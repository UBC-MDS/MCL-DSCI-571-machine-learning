---
params:
  dynamictitle: "module1_01"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module1/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from image_classifier import classify_image

from IPython.display import HTML, display
from PIL import Image, ImageFile
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

```


type: slides

# What is Supervised Machine Learning?

Notes: <br>

---

## Prevalence of Machine Learning (ML)


<center><img src='/module1/examples.png'  width = "75%" alt="404 image" /></center>


Notes: 

Machine learning (ML) is used often in our everyday lives. 

Some examples include:

- Voice assistance
- Google news
- Recommender Systems
- Face recognition
- Auto completion
- Stock market predictions
- Character recognition
- Self-driving cars
- Cancer diagnosis
- Drug Discovery
- AlphaGo 

--- 

## What is Machine Learning?

* A field of study that gives computers the ability to learn without being explicitly programmed.*
<br>      -- Arthur Samuel (1959)


<center>
<img src="/module1/traditional-programming-vs-ML.png" height="800" width="800"> 
</center>


Notes: 

So what exactly is machine learning?    

According to Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, it is: 

*"A field of study that gives computers the ability to learn without being explicitly programmed."*

We see it as a different way to think about problem-solving.

---

## Some concrete examples of supervised learning

<br>
<br>

## Example 1: Predict whether a patient has a liver disease or not

*In all the the upcoming examples, Don't worry about the code. Just focus on the input and output in each example.*

Notes: To introduce the capabilities of machine learning, we are going to show you a few examples. 

The first example is being able to predict whether a patient has a liver disease or not.

---

```{python include=FALSE}
df = pd.read_csv("data/indian_liver_patient.csv")
df = df.drop("Gender", axis=1)
df.loc[df["Dataset"] == 1, "Target"] = "Disease"
df.loc[df["Dataset"] == 2, "Target"] = "No Disease"
df = df.drop("Dataset", axis=1)
train_df, test_df = train_test_split(df, test_size=4, random_state=16)
train_df.head()
```


```{python}
train_df, test_df = train_test_split(df, test_size=4, random_state=16)
train_df.head()
```


Notes: First we obtain our data from our patients, wrangle it as necessary and split it up.


---


```{python  results = 'hide'}
from xgboost import XGBClassifier
X_train = train_df.drop(columns=['Target'], axis=1)
y_train = train_df['Target']
X_test = test_df.drop(columns=['Target'], axis=1)
model = XGBClassifier()
model.fit(X_train, y_train)
```




Notes: Next, we build a model and train our model using the labels we already have. 

Ignore this output here.  It's just explaining what's going on in the model which we will explain soon. 

---

```{python}
pred_df = pd.DataFrame(
    {"Predicted label": model.predict(X_test).tolist()}
)
df_concat = pd.concat([X_test.reset_index(drop=True), pred_df], axis=1)
df_concat
```


Notes: Next, we take our model and use it to predict on unseen data. 

Here we can see that our model is predicting an outcome of the patients under the `Predicted label` column. 

---

<br>
<br>
<br>

## Example 2: Predict the label of a given image

Notes: We can also use it for image recognition. 

---

##  Predict labels with associated probability scores for unseen images  

```{python eval=FALSE}
images = glob.glob("test_images/*.*")
for image in images:
    img = Image.open(image)
    img.load()
    plt.imshow(img)
    plt.show()
    df = classify_image(img)
    print(df.to_string(index=False))
```

<img src="/module1/module1_01/unnamed-chunk-6-1.png" width="300" />

```out
  Class  Probability
      ox     0.869893
  oxcart     0.065034
  sorrel     0.028593
 gazelle     0.010053

```


Notes: 
Here, we already have a trained model that has been shown hundreds of thousands of images. 

If we give it images from our own collection, the model attempts to make a prediction of the contents of the image. 

In this case, the model predicts the animal to be an `ox` with a probability score 86%. That's not bad. 

---


```{python eval=FALSE}
images = glob.glob("test_images/*.*")
for image in images:
    img = Image.open(image)
    img.load()
    plt.imshow(img)
    plt.show()
    df = classify_image(img)
    print(df.to_string(index=False))
```

<img src="/module1/module1_01/unnamed-chunk-6-2.png" width="300" />

```out
            Class  Probability
            llama     0.123625
               ox     0.076333
           kelpie     0.071548
 ibex, Capra ibex     0.060569

```


Notes: 

In this case, the model is much less confident in identifying the animal. The model gives the highest probability score that our donkey image, is a llama but only with a value of `0.12`. 

---

<br>
<br>
<br>

## Example 3: Predict sentiment expressed in a movie review (pos/neg)
*Attribution: The dataset `imdb_master.csv` was obtained from <a href="https://www.kaggle.com/uttam94/imdb-mastercsv" target="_blank">Kaggle</a>*

Notes: We also use machine learning to predict negative or positive sentiment expressed in a movie review.

---

```{python include=FALSE, Eval=FALSE}
imdb_df = pd.read_csv("data/imdb_master.csv", encoding="ISO-8859-1")
imdb_df = imdb_df[imdb_df["label"].str.startswith(("pos", "neg"))]
imdb_df = imdb_df.drop(["Unnamed: 0", "type"], axis=1)
train_df, test_df = train_test_split(imdb_df, test_size=0.10, random_state=12)

```


```{python}
train_df.head()
```

```out
                                                review label         file
43020  Just caught it at the Toronto International Fi...   pos   3719_9.txt
49131  The movie itself made me want to go and call s...   pos  9219_10.txt
23701  I came across this movie on DVD purely by chan...   pos   8832_9.txt
4182   Having seen Carlo Lizzani's documentary on Luc...   neg   2514_4.txt
38521  I loved this film. I first saw it when I was 2...   pos  1091_10.txt
```

Notes:  First we wrangle our data so that we can train our model. 

This data contains the review in a column named `review` and a `label` column which contains values of either `pos` or `neg` for positive or negative. 

---

```{python eval=FALSE}
X_train, y_train = train_df['review'], train_df['label']
X_test, y_test = test_df['review'], test_df['label']

clf = Pipeline(
    [
        ("vect", CountVectorizer(max_features=5000)),
        ("clf", LogisticRegression(max_iter=5000)),
    ]
)
clf.fit(X_train, y_train)
```


Notes: Next, we build our model and train on our existing data.

Again, don't worry about the code here. 

---

```{python eval=FALSE}
pred_dict = {
    "reviews": X_test[0:4],
    "true_sentiment": y_test[0:4],
    "sentiment_predictions": clf.predict(X_test[0:4]),
}
pred_df = pd.DataFrame(pred_dict)
pred_df.head()
```

```out
                                                 reviews prediction sentiment_predictions
43020  Just caught it at the Toronto International Fi...        pos                   pos
49131  The movie itself made me want to go and call s...        pos                   pos
23701  I came across this movie on DVD purely by chan...        pos                   pos
4182   Having seen Carlo Lizzani's documentary on Luc...        neg                   neg
```

Notes: Once we have our model trained, we can then predict data we haven't seen before using the model we just built. 

This we can see that in these 4 observations, the model correctly predicts each review's sentiment.

---

<br>
<br>
<br>

## Example 4: Predict housing prices
*Attribution: The dataset `kc_house_data.csv` was obtained from <a href="https://www.kaggle.com/harlfoxem/housesalesprediction" target="_blank">Kaggle</a>.*

Notes: Machine learning can also be used to predict housing prices.

---

```{python}
df = pd.read_csv("data/kc_house_data.csv")
df.drop(["id", "date"], axis=1, inplace=True)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=4)
train_df.head()
```


Notes: We wrangle our data just as we did before. 

These data consist of the characteristics of houses in King County, USA. 

---

```{python}

X_train = train_df.drop("price", axis=1)
X_train.head()
```

```{python}
y_train = train_df["price"]
y_train.head()
```

```{python}
X_test = test_df.drop("price", axis=1)
y_test = train_df["price"]
```



Notes: 

It's important that we separate our data from the target column (The `y` variables).

---


```{python results = 'hide'}
from xgboost import XGBRegressor

model = XGBRegressor()
model.fit(X_train, y_train)
```

Notes: We build our model. 

---


```{python}
pred_df = pd.DataFrame(
    {"Predicted price": model.predict(X_test[0:4]).tolist(), "Actual price": y_test[0:4].tolist()}
)
df_concat = pd.concat([X_test[0:4].reset_index(drop=True), pred_df], axis=1)
df_concat.head()
```


Notes: And we predict on unseen examples using the built model.

If we scroll to right, we can compare the actual price of the house and the price our model predicted. 

---

# Let’s apply what we learned!

Notes: <br>