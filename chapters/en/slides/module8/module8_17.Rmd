---
params:
  dynamictitle: "module8_17"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module8/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets
from sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression

pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module8/"
```


type: slides

# Multi-class classification

Notes: <br>

---

```{python}
data = datasets.load_wine()
X = pd.DataFrame(data['data'], columns=data["feature_names"])
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)
```

```{python}
X_train
```

```{python}
y_train
```


Notes: 

The classification problems we have looked at so far in this module have had binary labels (2 possible labels).

But we've seen that target labels are not restricted to this.

Often we will have classification problems where we have multiple labels such as this wine dataset we are going to use.

Here we have 3 classes for our target: 0, 1, 2 (maybe red, white and rose?).

---

```{python}
lr = LogisticRegression(max_iter=10000)
lr.fit(X_train, y_train);
```

```{python}
lr.predict(X_test[:5])
```


```{python}
lr.coef_
```

```{python}
lr.coef_.shape
```

Notes: 

For some models, like decision trees, we don't have to think about anything differently at all, but for our linear classifier, on the other hand, things are a bit different. 

Let's make a Logistic Regression here and look at the coefficients. (Ignore the `max_iter` for now. You can look into it <a href="https://medium.com/analytics-vidhya/a-complete-understanding-of-how-the-logistic-regression-can-perform-classification-a8e951d31c76" target="_blank">here</a> if you like)

What is going on here?

---


```{python}
lr_coefs = pd.DataFrame(data=lr.coef_.T, index=X_train.columns, columns=lr.classes_)
lr_coefs
```


Notes: 

What's happening here is that we have one coefficient per feature *per class*.

The interpretation is that these coefficients contributes to the predicting a certain class. 

The specific interpretation depends on the way the logistic regression is implementing multi-class.

---



```{python}
lr.predict_proba(X_test)[:5]
```

```{python}
lr.predict_proba(X_test[:5]).sum(axis=1)
```


Notes: 

If we look at the output of `predict_proba` you'll also see that there is a probability for each class and each row adds up to 1 as we would expect (total probability = 1).


---

```{python}
confusion_matrix(y_test, lr.predict(X_test))
```

```python
plot_confusion_matrix(lr, X_test, y_test, display_labels=lr.classes_, cmap='Blues', values_format='d');
```


```{python echo=FALSE, fig.width = 10, fig.height = 8,  out.width = '40%', fig.align='center'}
plt.rcParams.update({'font.size': 16})
plot_confusion_matrix(lr, X_test, y_test, display_labels=lr.classes_, cmap='Blues', values_format='d');
plt.show()
```


Note: As we saw in Module 7, we can still create confusion matrices but now they are greater than a 2 X 2 grid. 

We have 3 classes for this data, so our confusion matrix is 3 X 3. 

---


```{python}
print(classification_report(y_test, lr.predict(X_test)))
```

Notes: 

Precision, recall, etc. don't apply directly but like we said before, if we pick one of the classes as positive, and consider the rest to be negative, then we can.


---

```{python}
x_train_2d = X_train[['alcohol', 'malic_acid']]
x_train_2d.head(3)
```



```{python  echo=FALSE, fig.width = 13, fig.height = 8,  out.width = '48%', fig.align='center'}
lr_2d = LogisticRegression()
lr_2d.fit(x_train_2d, y_train);
plot_classifier(x_train_2d, y_train, lr_2d,  ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 18);
plt.yticks(fontsize= 18);
plt.xlabel("alcohol", fontsize=18); plt.ylabel("malic_acid", fontsize=18)
plt.title("Logistic Regression - Multi Class", fontsize=20)
```



Notes: 

We can also make plots shoulding the decision boundaries, but this time, it will show more classes.

For us to be able to plot this we need to select 2 features so we are picking `alcohol` and `malic_acid`. 

In this plot, the colours are inconsistent with the shapes. 
- The red triangles correspond to the light blue predictions.
- The black X's correspond to the red predictions/
- The blue circles (correctly) correspond to the blue circles.


---


```{python  echo=FALSE, fig.width = 12, fig.height = 8,  out.width = '80%', fig.align='center'}
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1);
rf_2f = DecisionTreeClassifier();
rf_2f.fit(X_train.iloc[:,:2], y_train);
plot_classifier(x_train_2d, y_train, rf_2f, ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("alcohol", fontsize=14); plt.ylabel("malic_acid", fontsize=14)
plt.title("Decision Tree - Multi Class", fontsize=16)
plt.subplot(1, 2, 2);
svm_2f = SVC(gamma=2, C=100)
svm_2f.fit(X_train.iloc[:,:2], y_train);

plot_classifier(x_train_2d, y_train, svm_2f, ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("alcohol", fontsize=14); plt.ylabel("malic_acid", fontsize=14)
plt.title("SVM - Multi Class", fontsize=16)

```


Notes: 

We can plot multi-class problems with other classifiers too. 

Here we can see the boundaries of the decision tree classifier as well as SVM with an RBF kernel. 

---


# Letâ€™s apply what we learned!

Notes: <br>
