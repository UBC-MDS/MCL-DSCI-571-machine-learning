---
params:
  dynamictitle: "module2_08"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module2/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module2/"

```


type: slides

# Decision trees with continuous features

Notes: <br>

---

```{python}
classification_df = pd.read_csv("data/quiz2-grade-toy-classification.csv")
classification_df.head()
```

```{python}
X = classification_df.drop(["quiz2"], axis=1)
X.head()
```

```{python include=FALSE}
y = classification_df["quiz2"]

```



Notes: 

We saw how the decision tree works when our data is binary but we know that this won't always happen. 

What happens when our features are continuous like in our quiz2 data?

All our features here have a numerical value. What do we do?

--- 

```{python include=FALSE}
import graphviz
# Trees with continuous features
model = DecisionTreeClassifier()
model.fit(X, y)
graphviz.Source(
    export_graphviz(
        model,
        out_file=None,
        feature_names=X.columns,
        class_names=["A+", "not A+"],
        impurity=False,
    )
).render("../../../../static/module2/module2_08a", format='png') 
```

<center><img src="/module2/module2_08a.png"  width = "60%" alt="404 image" /></center>

Notes: 


---

```{python}
X_subset = X[["lab4", "quiz1"]]
X_subset.head()
```


Notes: 

For the next example let's consider a subset of the data with only two features.

---

## Decision boundaries

```{python results='hide'}
depth = 1
model = DecisionTreeClassifier(max_depth=depth)
model.fit(X_subset, y)
```

```{python include=FALSE}
# Also called a decision stump

graphviz.Source(
    export_graphviz(
        model,
        out_file=None,
        feature_names=X_subset.columns,
        class_names=["A+", "not A+"],
        impurity=False,
    )
).render("../../../../static/module2/module2_08b", format='png') 
```

<center><img src="/module2/module2_08b.png"  width = "50%" alt="404 image" /></center>
    
Notes: 

What do we do with learned models? 

So far we have been using them to predict the class of a new instance. 

Another way to think about them is to ask: what sort of test examples will the model classify as positive, and what sort will it classify as negative? 
    
Here we can look at this ***decision stump*** which will show us where the first feature (`lab4`) makes a divide between an `A+` and `Not A+`. 

---

```{python echo=FALSE}
plt.figure(figsize=(4, 4))
plot_classifier(X_subset.to_numpy(), y.to_numpy(), model)
plt.title("Decision tree with depth = %d" %(depth))
```


Notes: 

We can assume a geometric view of the data. (More on this soon)

Here the red region corresponds to the "not A+" class and the blue region corresponds to the "A+" class. 

There is a line separating the red region and the blue region which is called the **decision boundary** of the model.

In our current model, this decision boundary is created by asking one question. 


---

### Another example of decision boundaries


```{python}
df = pd.read_csv('data/canada_usa_cities.csv')
df.head()
```



Notes: 

Here is another example of a decision boundary which can help explain the concept more visually. 

We have the latitude and longitude locations of different cities. 

We want to predict if they are Canadian or American cities using these features.

---

```{python}
canada = df[df['country'] == 'Canada']
usa = df[df['country'] == 'USA']
```

```{python}
chart1 = alt.Chart(df).mark_circle(size=20, opacity=0.6).encode(
    alt.X('longitude:Q', scale=alt.Scale(domain=[-140, -40]), axis=alt.Axis(grid=False)),
    alt.Y('latitude:Q', scale=alt.Scale(domain=[20, 60]), axis=alt.Axis(grid=False)),
    alt.Color('country:N', scale=alt.Scale(domain=['Canada', 'USA'],
                                           range=['red', 'blue']))
)
chart1
```

```{python include =FALSE}
chart1.save(path + 'chart1.png')
```

<img src="/module2/chart1.png" alt="A caption" width="50%" />


Notes: 


---

## Real boundary between Canada and USA

<center>
<img src="/module2/canada-us-border.jpg" width="100%"> 
</center>
**Attribution:**  <a href="https://sovereignlimits.com/blog/u-s-canada-border-history-disputes" target="_blank">sovereignlimits.com</a> 

Notes: 

---

# Letâ€™s apply what we learned!

Notes: <br>
