---
params:
  dynamictitle: "module2_01"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module2/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

```


type: slides

# Decision Tree Classifiers 

Notes: <br>

---

## Improving the baseline model



Examples:

<center><img src="/module2/quiz2-grade-toy.png"  width = "85%" alt="404 image" /></center>


Notes: 

So we have built a baseline model, but can we do better than that? 

If you are asked to write a program to predict whether a student gets an A+ or not in quiz2, how would you go for it? 

Before we go forward, let's try to make things a little simpler and binarize the input data.



--- 

## A program for prediction using a set of rules with *if/else* statements 


<center>
<img src="/module2/quiz2-grade-toy.png" height="600" width="600"> 
</center>


- How about a rule-based algorithm with several *if/else* statements?  
```
if class attendance == 1 and quiz1 == 1:
    quiz2 == "A+"
elif class attendance == 1 and lab3 == 1 and lab4 == 1:
    quiz2 == "A+"
...
```

Notes: 

Now that we have a model where our features are 1 of 2 options, how about we have a rule-based algorithm with several *if/else* statements?  
We learned about conditions and *if/else* statements in module 5 of ***Programming in Python for Data Science***, now let's incorporate this concept into a machine learning problem. 
For example: 

```
if class attendance == 1 and quiz1 == 1:
    quiz2 == "A+"
elif class attendance == 1 and lab3 == 1 and lab4 == 1:
    quiz2 == "A+"
...
```

Decision tree gives us a way to do this!

---

```{python}
classification_df = pd.read_csv("data/quiz2-grade-toy-classification.csv")
classification_df.head(3)
```


```{python}
X = classification_df.drop(["quiz2"], axis=1)
X.head(3)
```

```{python}
y = classification_df["quiz2"]
y.head(3)
```


Notes: 

Let's take our 𝑋 and 𝑦 from our `quiz2` data that we had before.


---

```{python}
X_binary = X.copy()
columns = ["lab1", "lab2", "lab3", "lab4", "quiz1"]
for col in columns:
    X_binary[col] = X_binary[col].apply(
        lambda x: 1 if x >= 90 else 0)
X_binary.head()    
```


Notes: 

Now let's binarize the features in `X` as we discussed. We can see that each column now only has a value of either `0` or `1`. 

Now we have our data in a preferred way, how do we make predictions with the `if`/`else` statements we talked about?

---

## Decision trees


<center><img src="/module2/nature.png"  width = "85%" alt="404 image" /></center>


Notes: 

The decision tree models use an algorithm that derives such rules from data in a principled way.

---

## Decision trees terminology 


<center>
<img src="/module2/lingo_tree.png"  width = "85%" alt="404 image">
</center>

Note: 


A tree is a type of structure with branches and nodes that is an effective way to visualize the process of decision making. 

A tree begins at the top which is called the ***root***.

Each decision is called a ***node*** and they are connected by ***branches***. 

With the decision tree algorithm in machine learning, the tree can have at most two **nodes** resulting from it, also known as **children**. 

A Decision Tree that only results in 2 children for each node takes on a specific named called a **Binary Decision Tree**. 

The maximum depth of a tree is somewhat like the "height" or how "tall" a tree stands. It refers to the length of the longest path from a root to a leaf.


---

<center>
<img src="/module2/example3.png"  width = "85%" alt="404 image">
</center>


Note: 

Using our quiz2 dataset as an example, a tree may look something like this. 

This tree has a depth of 3. 


---

## Decision Stump

<br>
<br>
<center>
<img src="/module2/stump.png"  width = "60%" alt="404 image">
</center>


Note: 

This tree has a depth of 1. 

A decision tree that has a depth of 1 is called a ***Decision stump***.  


---

# Let’s apply what we learned!

Notes: <br>
