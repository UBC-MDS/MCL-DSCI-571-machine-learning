---
params:
  dynamictitle: "module7_09"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module7/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, plot_confusion_matrix, plot_precision_recall_curve, precision_score, recall_score, f1_score


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module7/"
```


type: slides

# Multi-class measurements

Notes: <br>

---


```{python include=FALSE}
cc_df = pd.read_csv('data/creditcard.csv', encoding='latin-1')
train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)

X_train_big, y_train_big = train_df.drop(columns=["Class"]), train_df["Class"]
X_test, y_test = test_df.drop(columns=["Class"]), test_df["Class"]

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)
                                                      
pipe_tree = make_pipeline(
    (StandardScaler()),
    (DecisionTreeClassifier(random_state=123))
)
pipe_tree.fit(X_train,y_train);
predictions = pipe_tree.predict(X_valid)


```

```{python echo=FALSE, fig.width = 8, fig.height = 5,  out.width = '45%', fig.align='center'}
plt.rcParams.update({'font.size': 14})
plot_confusion_matrix(pipe_tree, X_valid, y_valid, display_labels=["Non fraud", "Fraud"], values_format="d", cmap="Blues");
plt.show()
```

```{python}
print(classification_report(y_valid, pipe_tree.predict(X_valid),
        target_names=["non-fraud", "fraud"]))
```

Notes: 

Right now, we have only seen measurements about target columns with binary values. 

What happens when we have a target with more than 2 classes?

---

```{python}
from sklearn.datasets import load_digits
from sklearn.metrics import accuracy_score
```


```{python}
digits = load_digits()
digits.images[-1]
```


```{python echo=FALSE,  out.width = '40%'}
plt.figure(1, figsize=(3, 3))
plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')
plt.show()
```


Notes: 

This time we are going to look at a dataset of images.

In this case, each image is a hand-written digit (0-9). 

The data for a single image is represented by a matrix that is shaped  8 by 8. This corresponds to each pixel of the image.


---

```{python}
X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(
    digits['data'] / 16., digits['target'], random_state=0)
    
knn = KNeighborsClassifier().fit(X_train_digits, y_train_digits)
pred = knn.predict(X_test_digits)
print("Accuracy: ", accuracy_score(y_test_digits, pred).round(4))

```


Notes: 

We are going to do the same thing we've always done and predict the digit by splitting our data. 

In this case, our `X` is the column `data` and our target is the column `target`. 

We use a `KNeighborsClassifier` to fit and predict our accuracy using `accuracy_score()`. 

Here we get an accuracy of 98%. 

But what does this mean for our metrics?

---

## Confusion matrix for multi-class

```{python fig.width = 8, fig.height = 6,  out.width = '55%'}
plot_confusion_matrix(knn, X_test_digits, y_test_digits, cmap='gray_r');
plt.show()
```


Notes: 

We see that we can still compute a confusion matrix, for problems with more than 2 labels in the target column. 

The diagonal values are the correctly labeled digits and the rest are the errors. 


---

```{python}
print(classification_report(y_test_digits, pred, digits=4))
```


Notes: 

This time, we have different precision and recall values depending on which digit we specify as our "positive" label. 

Again the `support` column on the right shows the number of examples of each digit. 

What about this `macro avg` and `weight avg` we see on the bottom? 

What are these? 


---

### Macro average vs weighted average


```{python}
print(classification_report(y_test_digits, pred, digits=4))
```


**Macro average:** Give equal importance to all classes.  

**Weighted average:** Weighted by the number of samples in each class and divide by the total number of samples.     



Notes: 

We saw them before when we were using binary-class problems but these metrics are more useful when predicting multiple classes. 

**Macro average** is useful when you want to give equal importance to all classes irrespective of the number of instances in each class.

**Weighted average** gives equal importance to all examples. So, when you care about the overall score and do not care about the score on a specific class, you could use it.

Which one is relevant, depends upon whether you think each class should have the same weight or each sample should have the same weight. 

---

# Letâ€™s apply what we learned!

Notes: <br>
