---
params:
  dynamictitle: "module7_16"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module6/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import make_column_transformer


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module7/"
```


type: slides

# Regression Measurements 

Notes: <br>

---

```{python}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
```

```{python include = FALSE}
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
```

```{python}
X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]
X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]

numeric_features = [ "longitude", "latitude",
                     "housing_median_age",
                     "households", "median_income",
                     "rooms_per_household",
                     "bedrooms_per_household",
                     "population_per_household"]
                     
categorical_features = ["ocean_proximity"]

X_train.head()
```



Notes: 

This next section involves looking at regression problems and so we are going to bring back our California housing dataset where we want to predict the median house value for different locations. 


---

```{python}
numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), 
           ("scaler", StandardScaler())]
)

categorical_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
           ("onehot", OneHotEncoder(handle_unknown="ignore"))]
)

preprocessor = make_column_transformer(
(numeric_transformer, numeric_features),
        (categorical_transformer, categorical_features), 
    remainder='passthrough')

pipe = make_pipeline(preprocessor, KNeighborsRegressor())
pipe.fit(X_train, y_train);
```



Notes: 

We are going to bring in our previous pipelines and fit our model. 




---

```{python}
predicted_y = pipe.predict(X_train) 
```



```{python}
predicted_y == y_train
```


```{python}
y_train.values
```


```{python}
predicted_y
```


Notes: 

We aren't doing classification anymore, so we can't just check for equality. 

We need a score that reflects how right/wrong each prediction is.

How close we are to the actual numeric value?

---

## Regression measurements 

The scores we are going to discuss are: 

- mean squared error (MSE)
- R^2
- root mean squared error (RMSE)
- MAPE


If you want to see these in more detail, you can refer to the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" target="_blank">sklearn documentation</a>.

Notes: 



---

### Mean squared error (MSE)

<center><img src="/module7/mse.svg"  width = "20%" alt="404 image" /></center>

<center><img src="/module7/mse-easy.svg"  width = "38%" alt="404 image" /></center>

```{python}
predicted_y
```


```{python}
np.mean((y_train - predicted_y)**2)
```
```{python}
np.mean((y_train - y_train)**2)
```


Notes: 

mean squared error is a common measure.

We calculate this by calculating the difference between the predicted and actual value, square it and sum all these values for every example in the data. 

Perfect predictions would have MSE=0.

---


```{python}
from sklearn.metrics import mean_squared_error 
```
```{python}
mean_squared_error(y_train, predicted_y)
```


Notes: 

We can use `mean_squared_error` from sklearn again instead of calculating this ourselves. 


If we look at MSE here, it's huge and unreasonable. 

Is this score good or bad?

Unlike classification, in regression, our target has units. 

In this case, our target column is the median housing value which is in dollars. 

That means that the mean squared error is in dollars^2. 

The score also depends on the scale of the targets. 

If we were working in cents instead of dollars, our MSE would be 10,000 X (100^2) higher!


---

### R^2 (quick notes)

Key points:

- The maximum value possible is 1 which means the model has perfect predictions.
- Negative values are very bad: "worse than baseline models such as`DummyRegressor`". 


```{python}
from sklearn.metrics import r2_score
```


Notes: 


This is the score that `sklearn` uses by default when you call `.score()` so we've already seen R^2 when in our regression problems. 

You can <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank">read about it here</a> but we are going to just give you the quick notes. 

Intuition: mean squared error, but flipped where higher values mean a better measurement.

Normalized so the max is 1.

When you call `fit` it minimizes MSE / maximizes R^2 (or something like that) by default.

Just like in classification, this isn't always what you want.



---


```python
print(mean_squared_error(y_train, predicted_y))
print(mean_squared_error(predicted_y, y_train))
```

```out
2570054492.048064
2570054492.048064
```

```python
print(r2_score(y_train, predicted_y))
print(r2_score(predicted_y, y_train))
```

```out
0.8059396097446094
0.742915970464153
```


Notes: 

We can reverse MSE but not R^2 (optional).

---

### Root mean squared error  (RMSE)

<center><img src="/module7/rmse-simp.svg"  width = "20%" alt="404 image" /></center>

<center><img src="/module7/mse-easy.svg"  width = "38%" alt="404 image" /></center>


```{python}
mean_squared_error(y_train, predicted_y)
```

```{python}
np.sqrt(mean_squared_error(y_train, predicted_y))
```



Notes: 

The MSE we had before was in dollars^2.

A more relatable metric would be the root mean squared error, or RMSE. 

This now has the units in dollars.  Instead of 250 million dollars squared our error measurement is around $50,000.

---



```{python}

```

```{python  echo=FALSE,  fig.width = 13, fig.height = 9,  out.width = '75%', fig.align='center'}
df = pd.DataFrame(y_train).assign(predicted = predicted_y).rename(columns = {'median_house_value': 'true'})
df = pd.DataFrame(y_train).assign(predicted = predicted_y).rename(columns = {'median_house_value': 'true'})
plt.scatter(y_train, predicted_y, alpha=0.3, s = 10)
grid = np.linspace(y_train.min(), y_train.max(), 1000)
plt.plot(grid, grid, '--k');
plt.xticks(fontsize= 12);
plt.yticks(fontsize= 12);
plt.xlabel("true price", fontsize=14);
plt.ylabel("predicted price", fontsize=14)
```


Notes: 

When we plot our predictions versus the examples' actual value, we can see cases where our prediction is way off.

Under the line means we're under-prediction, over the line means we're over-predicting.

Question: Is an error of $30,000 acceptable?     

- For a house worth $600k, it seems reasonable! That's a 5% error.
- For a house worth $60k, that is terrible. It's a 50% error.

---

### MAPE - Mean Absolute Percent Error (MAPE)


```{python}
percent_errors = (predicted_y - y_train)/y_train * 100.
percent_errors.head()
```

```{python}
np.abs(percent_errors).head()
```
```{python}
100.*np.mean(np.abs((predicted_y - y_train)/y_train))
```


Notes: 

So, finding the percentage error may be handy, can we compute something like that?

We can calculate a percentage error for each example. Now the errors are both positive (predict too high) and negative (predict too low).

We can look at the absolute percent error which now shows us how far off we were direction not dependent. 

Like MSE, we can take the average over all the examples. This is called **Mean Absolute Percent Error (MAPE)**.

Ok, this is quite interpretable. We can see that on average, we have around 18% error in our predicted median housing valuation.

---

# Let’s apply what we learned!

Notes: <br>
