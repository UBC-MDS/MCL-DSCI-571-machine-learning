---
params:
  dynamictitle: "module5_05"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module5/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module5/"
```


type: slides

# Preprocessing with imputation

Notes: <br>

---

## Case study: California housing prices

```{python}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)

train_df.head()
```
 We are using the data that can be <a href="https://www.kaggle.com/harrywang/housing" target="_blank">downloaded here</a>. 
This dataset is a modified version of the California Housing dataset available from: <a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" target="_blank">Luís Torgo's University of Porto website</a>.





Notes: 

For the next few slide decks we are going to be using a dataset exploring the prices of homes in California to demonstrate feature transformation techniques. 

The task is to predict median house values in Californian districts, given a number of features from these districts. 

We can see here thatome column values are mean/median while others are totals or not completely clear. 


---

```{python}
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"])
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"])

train_df = train_df.assign(bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"])
test_df = test_df.assign(bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"])

train_df = train_df.assign(population_per_household = train_df["population"]/train_df["households"])
test_df = test_df.assign(population_per_household = test_df["population"]/test_df["households"])

train_df.head()
```


Notes: 

Let's add some new features to the dataset which could help predict the target: `median_house_value`.

**When is it OK to do things before splitting?**

Here it would have been OK to add new features before splitting because we are not using any global information in the data but only looking at one row at a time. 

But just to be safe and to avoid accidentally breaking the golden rule, it's better to do it after splitting. 

**Question**: Should we remove `total_rooms`, `total_bedrooms`, and `population` columns? 

Probably, but let's keep them in for now. You could experiment with removing them and examine whether results change. 


---

## Exploratory Data Analysis (EDA)

```{python}
train_df.info()
```


Notes: 

Here we see that we have all columns with Dtype `float64` except for `ocean_proximity` which appears categorical. 

It looks like  the `total_bedrooms` column has some missing values.


---



```{python}
train_df.describe()
```
```{python}
housing_df["total_bedrooms"].isnull().sum()
```

Notes: 

This was the same column we used to engineer the `bedrooms_per_household` column and it look like it's been effected as well. 

---

### So what do we do? 


```{python}
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
```


```python
knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
```
```out
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
```

Notes: 

First we are going to drop the categorical variable `ocean_proximity`.  

Right now we only know how to build models with numerical data. We will come back to categorical variables in module 6. 

But what do we do about the null values? Can we run a model with them? 

We can see that the classifier is not able to deal with missing values (NaNs).

What are possible ways to deal with the problem? 


---

## Dropping


```{python}
housing_df["total_bedrooms"].isnull().sum()
```

```{python}
X_train.shape
```

```{python}
X_train_no_nan = X_train.dropna()
y_train_no_nan = y_train.dropna()
```

```{python}
X_train_no_nan.shape
```



Notes: 

We could drop the rows but we'd need to do the same in our test set.

That also doesn't help us if we get a missing value in deployment. What do we do then? 

Furthermore, what if the missing values don't occur at random and we're systematically dropping certain data?

This is not a great solution, especially if there's a lot of missing values.

---

## Dropping a column

```{python}
X_train.shape
```

```{python}
X_train_no_col = X_train.dropna(axis=1)
```

```{python}
X_train_no_col.shape
```

Notes:

One can also drop all columns with missing values.

This generally throws away a lot of information, because you lose a whole column just for 1 missing value.
But I might drop a column if it's 99.9% missing values, for example.


---

## Imputation

**Imputation**: Imputation means inventing values for the missing data.


```{python}
from sklearn.impute import SimpleImputer
```


We can impute missing values in:

- **Categorical columns**: with the most frequent value.
- **Numeric columns**:  with the mean or median of the column.    

Notes: 

`SimpleImputer` is a **transformer** in `sklearn` which can deal with this problem. 

We can going to concentrate on numeric columns in this section and address categorical preprocessing in Module 6. 

---

```{python}
X_train.sort_values('total_bedrooms').tail(10)
```


Notes: 

Let's take a look at the some of the examples with `NaN` values in `total_bedrooms`. 

Here we see that index `7763` has a `NaN` value for `total_bedrooms` and `bedrooms_per_household`.

---

```{python}
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train);
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)
```

```{python}
X_train_imp
```



Notes: 

Let's input our data and instead of dropping the examples likes replace the`fit` and `transform` steps that we saw earlier. We do not need to fit on our target column this time. 

Let's check whether the NaN values have been replaced or not.

Note that `imputer.transform` returns an `numpy` array and not a dataframe.

---

```{python}
df = pd.DataFrame(X_train_imp, columns = X_train.columns, index = X_train.index)
df.loc[[7763]]
```
```{python}
X_train.loc[[7763]]
```

Notes: 

We can fix that a bit using the column index from  `X_train` and transforming it into a dataframe. 

Now we can see our example 7763 no longer has an `NaN` value for `total_bedrooms` and `bedrooms_per_household`. 

---

```{python}
knn = KNeighborsRegressor();
knn.fit(X_train_imp, y_train)
knn.score(X_train_imp, y_train)
```


Notes: 

Can we train on the data with the new data `X_train_imp`?  

Yes! 

---


# Let’s apply what we learned!

Notes: <br>