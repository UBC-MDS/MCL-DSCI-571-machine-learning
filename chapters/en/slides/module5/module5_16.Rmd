---
params:
  dynamictitle: "module5_16"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module5/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module5/"
```


type: slides

# Automated hyperparameter optimization

Notes: <br>

---

## The problem with hyperparameters


- You may have a lot of them (e.g. deep learning).
- Picking reasonable hyperparameters is important -> avoid underfit or overfit model. 
- Nobody knows how to choose them.
- May interact with each other in unexpected ways.
- The best settings depend on the specific data/problem.
- Can take a long time.



    
    
Notes: <br>

---

## How to pick hyperparameters

### Manual hyperparameter optimization

**Advantages**:      
- We may have some intuition about what might work.

**Disadvantages**:        
- It takes a lot of work.
- In some cases intuition might be worse than a data-driven approach.


### Automated hyperparameter optimization 

**Advantages**:       
- Reduce human effort.
- Less prone to error.
- Data-driven approaches may be effective.

**Disadvantages**:   
- May be hard to incorporate intuition.
- Overfitting on the validation set. 

Notes: 


---

### Automated hyperparameter optimization


Exhaustive grid search: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" target="_blank">`sklearn.model_selection.GridSearchCV`</a>  
Randomized hyperparameter optimization: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" target="_blank">`sklearn.model_selection.RandomizedSearchCV`</a>  

Notes: 

There are two automated hyperparameter search methods in scikit-learn.

The "CV" stands for cross-validation; these searchers have cross-validation built right in.

---

## Bring back our California data

```{python}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)

train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])

test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"], 
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])


X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
```

```{python}
X_train
```



Notes: 

Let's bring back the California home data that we were using earlier in this model. 

This time we are dropping the columns `total_rooms`, `total_bedrooms` and `population`. 

---

## Exhaustive grid search


```{python}
from sklearn.model_selection import GridSearchCV
```


```{python}
pipe = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("reg", SVR()),
    ]
)
```

```{python}

param_grid = {
    "reg__gamma": [0.1, 1.0, 10, 100],
    "reg__C": [0.1, 1.0, 10, 100],
}
```


```{python}
grid_search = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True, verbose=2)
grid_search.fit(X_train, y_train);
```

```{python}
grid_search.best_score_

```


Notes: 

[`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

How does it work? 
- After specifying the steps in a pipeline , a user specifies a set of values for each hyperparameter. 

- The method considers the  product of the sets and then evaluates each combination one by one.    


---


Notes: 

---

<center><img src="/module2/module2_12b.png"  width = "50%" alt="404 image" /></center>


Notes: 

---


---

Notes: 

There are many other hyperparameters for decision trees you can explore at the link <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank">here</a> .


---

Notes: 

<br>

---

# Letâ€™s apply what we learned!

Notes: <br>